{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMADAUAM/t/2uXL5BporKFN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rashmiacekiper/Assignment-1/blob/main/AWS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q. 1 - Explain the difference between AWS Regions, Availability Zones, and Edge Locations. Why is this important for data analysis and latency-sensitive applications.\n",
        "A - Understanding the difference between AWS Regions, Availability Zones, and Edge Locations is crucial for designing scalable, fault-tolerant, and low-latency applications, especially in data analysis and latency-sensitive environments.\n",
        "________________________________________\n",
        " 1. AWS Regions\n",
        "‚Ä¢\tDefinition: A Region is a geographically distinct area that contains multiple, isolated Availability Zones.\n",
        "‚Ä¢\tExamples: us-east-1 (N. Virginia), eu-west-1 (Ireland), ap-southeast-1 (Singapore)\n",
        "‚Ä¢\tPurpose: Regions allow you to deploy resources close to your users or comply with data sovereignty laws.\n",
        "‚Ä¢\tImpact: Choosing the right Region affects latency, data residency, cost, and availability.\n",
        "________________________________________\n",
        "2. Availability Zones (AZs)\n",
        "‚Ä¢\tDefinition: An Availability Zone is a distinct location within a Region, with independent power, cooling, and networking.\n",
        "‚Ä¢\tEach Region typically has 2‚Äì6 AZs.\n",
        "‚Ä¢\tPurpose: AZs provide fault isolation. You can deploy applications across multiple AZs to ensure high availability and disaster recovery.\n",
        "‚Ä¢\tImpact: Using multiple AZs protects against data center failures and supports robust, fault-tolerant architecture for analytics systems.\n",
        "________________________________________\n",
        " 3. Edge Locations\n",
        "‚Ä¢\tDefinition: Edge Locations are global data centers used by AWS CloudFront (CDN) and AWS Global Accelerator to cache and deliver content closer to users.\n",
        "‚Ä¢\tPurpose: Minimize latency for static and dynamic content by serving from the location closest to the end-user.\n",
        "‚Ä¢\tImpact: For latency-sensitive applications (e.g., real-time dashboards, media streaming), Edge Locations reduce response times significantly.\n",
        "________________________________________\n",
        " Why This Matters for Data Analysis and Latency-Sensitive Applications\n",
        "Concern\tHow AWS Components Help\n",
        "Latency\tEdge Locations deliver fast responses to users globally.\n",
        "Availability & Reliability\tMulti-AZ deployment ensures systems stay online during failures.\n",
        "Scalability\tRegions allow for global infrastructure to scale analytics.\n",
        "Data Sovereignty/Compliance\tRegions allow control over where data is stored and processed.\n",
        "________________________________________\n",
        "Q. 2 - Using the AWS CLI, list all available AWS regions. Share the command used and the output.\n",
        "A - To list all available AWS Regions using the AWS CLI, you can use the following command:\n",
        "________________________________________\n",
        " Command:\n",
        "aws ec2 describe-regions --all-regions --query \"Regions[*].RegionName\" --output table\n",
        "________________________________________\n",
        " Explanation:\n",
        "‚Ä¢\tdescribe-regions: Lists all Regions that are available to your account.\n",
        "‚Ä¢\t--all-regions: Includes all enabled and disabled Regions.\n",
        "‚Ä¢\t--query: Filters the output to show only Region names.\n",
        "‚Ä¢\t--output table: Formats the output as a table for easier readability.\n",
        "________________________________________\n",
        " Example Output:\n",
        "---------------------------------\n",
        "|        DescribeRegions        |\n",
        "+-------------------------------+\n",
        "|  af-south-1                   |\n",
        "|  ap-east-1                    |\n",
        "|  ap-northeast-1              |\n",
        "|  ap-northeast-2              |\n",
        "|  ap-northeast-3              |\n",
        "|  ap-south-1                  |\n",
        "|  ap-south-2                  |\n",
        "|  ap-southeast-1              |\n",
        "|  ap-southeast-2              |\n",
        "|  ap-southeast-3              |\n",
        "|  ca-central-1                |\n",
        "|  eu-central-1                |\n",
        "|  eu-central-2                |\n",
        "|  eu-north-1                  |\n",
        "|  eu-south-1                  |\n",
        "|  eu-south-2                  |\n",
        "|  eu-west-1                   |\n",
        "|  eu-west-2                   |\n",
        "|  eu-west-3                   |\n",
        "|  me-central-1                |\n",
        "|  me-south-1                  |\n",
        "|  sa-east-1                   |\n",
        "|  us-east-1                   |\n",
        "|  us-east-2                   |\n",
        "|  us-west-1                   |\n",
        "|  us-west-2                   |\n",
        "+-------------------------------+\n",
        "________________________________________\n",
        "Q. 3 - Create a new IAM user with least privilege access to Amazon S3. Share your attached policies (JSON or screenshot)\n",
        "A - Sure! Here's a short version of how to create an IAM user with least privilege access to a specific S3 bucket:\n",
        "________________________________________ IAM User: s3-limited-user\n",
        "Grants access only to one bucket with basic list, get, put, and delete permissions.\n",
        "________________________________________\n",
        "Attached Policy (JSON)\n",
        "{\n",
        "  \"Version\": \"2012-10-17\",\n",
        "  \"Statement\": [\n",
        "    {\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Action\": [\"s3:ListBucket\"],\n",
        "      \"Resource\": \"arn:aws:s3:::example-bucket\"\n",
        "    },\n",
        "    {\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Action\": [\n",
        "        \"s3:GetObject\",\n",
        "        \"s3:PutObject\",\n",
        "        \"s3:DeleteObject\"\n",
        "      ],\n",
        "      \"Resource\": \"arn:aws:s3:::example-bucket/*\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "üîÅ Replace example-bucket with your actual bucket name.\n",
        "________________________________________\n",
        "Q. 4 - Compare different Amazon S3 storage (Standard, Intelligent-Tiering, Glacier). When should each be used in data analytics workflows?\n",
        "A - Here‚Äôs a short comparison of key Amazon S3 storage classes and when to use them in data analytics workflows:\n",
        "________________________________________\n",
        " S3 Storage Classes Comparison\n",
        "Storage Class\tUse Case\tAccess Frequency\tCost\tRetrieval Time\n",
        "S3 Standard\tFrequently accessed data\tHigh\t$$$\tMilliseconds\n",
        "S3 Intelligent-Tiering\tUnknown/variable access patterns\tVaries\t$$ (with tiering fee)\tMilliseconds to hours\n",
        "S3 Glacier\tLong-term archive, infrequent access\tVery low\t$\tMinutes to hours\n",
        "________________________________________\n",
        "When to Use in Data Analytics\n",
        "Workflow Stage\tRecommended Storage Class\tWhy\n",
        "Active data processing\tS3 Standard\tLow latency, high availability\n",
        "Logs/data with unpredictable use\tS3 Intelligent-Tiering\tAuto-optimizes cost based on access patterns\n",
        "Historical or archived datasets\tS3 Glacier\tCheap storage for cold data, retrieved occasionally\n",
        "\n",
        "Q. 5 - Create an S3 bucket and upload a sample dataset (CSV or JSON). Enable versioning and show at least two versions of one file.\n",
        "‚úÖ Short Version (AWS CLI)\n",
        "1. Create S3 Bucket\n",
        "aws s3api create-bucket --bucket my-sample-bucket-123 --region us-east-1\n",
        "2. Enable Versioning\n",
        "aws s3api put-bucket-versioning \\\n",
        "  --bucket my-sample-bucket-123 \\\n",
        "  --versioning-configuration Status=Enabled\n",
        "3. Create Sample CSV\n",
        "echo -e \"id,name\\n1,Alice\" > data.csv\n",
        "aws s3 cp data.csv s3://my-sample-bucket-123/data.csv\n",
        "4. Update File (New Version)\n",
        "echo -e \"id,name\\n1,Alice\\n2,Bob\" > data.csv\n",
        "aws s3 cp data.csv s3://my-sample-bucket-123/data.csv\n",
        "5. List File Versions\n",
        "aws s3api list-object-versions --bucket my-sample-bucket-123 --prefix data.csv\n",
        "This will show at least two versions of data.csv.\n",
        "\n",
        "## Q 6. Write and apply a lifecycle policy to move files to Glacier after 30 days and delete them after 90. Share the policy JSON or Screenshot.\n",
        "\n",
        "Ans - ### ‚úÖ S3 Lifecycle Policy (Short Version)\n",
        "\n",
        "#### üìù **Policy JSON**\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"Rules\": [\n",
        "    {\n",
        "      \"ID\": \"MoveToGlacierThenDelete\",\n",
        "      \"Status\": \"Enabled\",\n",
        "      \"Filter\": {\n",
        "        \"Prefix\": \"\"\n",
        "      },\n",
        "      \"Transitions\": [\n",
        "        {\n",
        "          \"Days\": 30,\n",
        "          \"StorageClass\": \"GLACIER\"\n",
        "        }\n",
        "      ],\n",
        "      \"Expiration\": {\n",
        "        \"Days\": 90\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### üöÄ **Apply Policy via AWS CLI**\n",
        "\n",
        "```bash\n",
        "aws s3api put-bucket-lifecycle-configuration \\\n",
        "  --bucket my-sample-bucket-123 \\\n",
        "  --lifecycle-configuration file://lifecycle.json\n",
        "```\n",
        "## Q 7. Compare RDS, DynamoDB, and Redshift for use in different stages of a data pipeline. Give one use case for each.\n",
        " Ans - ### ‚úÖ RDS vs DynamoDB vs Redshift ‚Äì For Data Pipeline Stages (Short)\n",
        "\n",
        "| Service      | Type              | Best For                          | Example Use Case                                |\n",
        "| ------------ | ----------------- | --------------------------------- | ----------------------------------------------- |\n",
        "| **RDS**      | Relational DB     | Structured transactional data     | **App database** for storing user data in OLTP  |\n",
        "| **DynamoDB** | NoSQL (Key-Value) | High-speed, scalable reads/writes | **Real-time ingestion** of IoT sensor data      |\n",
        "| **Redshift** | Data Warehouse    | Analytics, large-scale queries    | **BI/Reporting**: Analyze historical sales data |\n",
        "\n",
        "### üéØ Summary:\n",
        "\n",
        "* **RDS** ‚Üí Best for transactional workloads (OLTP).\n",
        "* **DynamoDB** ‚Üí Ideal for high-throughput, low-latency ingestion.\n",
        "* **Redshift** ‚Üí Designed for heavy analytics (OLAP) and reporting.\n",
        "\n",
        "\n",
        "## Q 8. Create a DynamoDB table and insert 3 records manually. Then write a Lambda function that adds records when triggered by S3 uploads.\n",
        "\n",
        "Ans - ### ‚úÖ Step-by-Step (Short Version)\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Create DynamoDB Table**\n",
        "\n",
        "```bash\n",
        "aws dynamodb create-table \\\n",
        "  --table-name MyDataTable \\\n",
        "  --attribute-definitions AttributeName=id,AttributeType=S \\\n",
        "  --key-schema AttributeName=id,KeyType=HASH \\\n",
        "  --billing-mode PAY_PER_REQUEST\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Insert 3 Records Manually**\n",
        "\n",
        "```bash\n",
        "aws dynamodb put-item --table-name MyDataTable --item '{\"id\":{\"S\":\"1\"}, \"name\":{\"S\":\"Alice\"}}'\n",
        "aws dynamodb put-item --table-name MyDataTable --item '{\"id\":{\"S\":\"2\"}, \"name\":{\"S\":\"Bob\"}}'\n",
        "aws dynamodb put-item --table-name MyDataTable --item '{\"id\":{\"S\":\"3\"}, \"name\":{\"S\":\"Charlie\"}}'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Lambda Function (Triggered by S3 Upload)**\n",
        "\n",
        "#### üìù Sample Lambda (Python)\n",
        "\n",
        "```python\n",
        "import boto3\n",
        "import uuid\n",
        "\n",
        "dynamodb = boto3.resource('dynamodb')\n",
        "table = dynamodb.Table('MyDataTable')\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    for record in event['Records']:\n",
        "        file_name = record['s3']['object']['key']\n",
        "        table.put_item(Item={\n",
        "            'id': str(uuid.uuid4()),\n",
        "            'name': file_name\n",
        "        })\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Connect Lambda to S3 Event**\n",
        "\n",
        "* In S3 bucket ‚Üí **Properties** ‚Üí **Event notifications**\n",
        "* Add event to trigger Lambda on **`PUT` (upload)**\n",
        "\n",
        "\n",
        "## Q 9.What is serverless computing? Discuss pros and cons of using AWS Lambda for data pipelines.\n",
        "\n",
        "Ans - ### ‚úÖ What is Serverless Computing?\n",
        "\n",
        "**Serverless computing** is a cloud execution model where the cloud provider **automatically manages infrastructure**‚Äîscaling, provisioning, and maintenance. You just write code; the platform runs it **on demand**, often in response to events.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è AWS Lambda for Data Pipelines\n",
        "\n",
        "**AWS Lambda** is a serverless function service ideal for event-driven tasks like S3 uploads, data transforms, or stream processing.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Pros of Using AWS Lambda in Data Pipelines\n",
        "\n",
        "| Advantage                | Description                                        |\n",
        "| ------------------------ | -------------------------------------------------- |\n",
        "| **No server management** | No provisioning or patching of servers needed.     |\n",
        "| **Auto-scaling**         | Instantly scales with workload spikes.             |\n",
        "| **Event-driven**         | Easily triggered by S3, DynamoDB, Kinesis, etc.    |\n",
        "| **Cost-effective**       | Pay only for compute time used (ms-level billing). |\n",
        "| **Quick deployment**     | Fast to build and deploy small, modular functions. |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùå Cons of Using AWS Lambda\n",
        "\n",
        "| Limitation            | Description                                                                           |\n",
        "| --------------------- | ------------------------------------------------------------------------------------- |\n",
        "| **Timeout limit**     | Max execution time is 15 minutes.                                                     |\n",
        "| **Cold starts**       | Initial requests may be slow if idle.                                                 |\n",
        "| **Limited compute**   | Memory & CPU limits restrict heavy processing.                                        |\n",
        "| **Complex debugging** | Harder to debug compared to monolithic apps.                                          |\n",
        "| **State handling**    | Stateless by design‚Äîexternal systems (like S3 or DynamoDB) needed for stateful logic. |\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Best Use in Data Pipelines:\n",
        "\n",
        "* **Ingestion**: Trigger on file upload (S3) or data stream (Kinesis).\n",
        "* **Transformation**: Clean/normalize data before storing in a database.\n",
        "* **Orchestration**: Light coordination using Step Functions.\n",
        "\n",
        "## Q 10.Create a Lambda function triggered by S3 uploads that logs file name, size, and timestamp to Cloudwatch. Share code and a log screenshot.\n",
        "\n",
        "Ans - ### ‚úÖ Lambda Function Triggered by S3 Upload (Short Version)\n",
        "\n",
        "#### üìù **Lambda Code (Python)**\n",
        "\n",
        "```python\n",
        "import json\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    for record in event['Records']:\n",
        "        s3_obj = record['s3']['object']\n",
        "        file_name = s3_obj['key']\n",
        "        file_size = s3_obj['size']\n",
        "        event_time = record['eventTime']\n",
        "        \n",
        "        logger.info(f\"New S3 Upload: File = {file_name}, Size = {file_size} bytes, Time = {event_time}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîó **Trigger Setup**\n",
        "\n",
        "* Go to your **S3 bucket** ‚Üí **Properties** ‚Üí **Event notifications**.\n",
        "* Add a notification:\n",
        "\n",
        "  * **Event type**: PUT\n",
        "  * **Destination**: Lambda function (choose the one above)\n",
        "\n",
        "\n",
        "## Q 11. Use AWS Glue to crawl your S3 dataset, create a Data Catalog table, and run a Glue job to convert CSV data to parquet. Share job code and output location.\n",
        "\n",
        "Ans - ### ‚úÖ AWS Glue: CSV ‚û°Ô∏è Parquet Conversion (Short Version)\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Upload CSV to S3**\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "s3://my-data-bucket/raw/customers.csv\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Create Glue Crawler**\n",
        "\n",
        "* **Source**: S3 path above\n",
        "* **Output**: Glue Data Catalog DB (e.g., `raw_db`)\n",
        "* **Table**: Created automatically (e.g., `customers`)\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Create Glue Job to Convert CSV to Parquet**\n",
        "\n",
        "#### üìù Glue Job Script (PySpark)\n",
        "\n",
        "```python\n",
        "import sys\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from awsglue.context import GlueContext\n",
        "from pyspark.context import SparkContext\n",
        "\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "\n",
        "# Load CSV table\n",
        "datasource = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=\"raw_db\", table_name=\"customers\"\n",
        ")\n",
        "\n",
        "# Write as Parquet\n",
        "glueContext.write_dynamic_frame.from_options(\n",
        "    frame=datasource,\n",
        "    connection_type=\"s3\",\n",
        "    connection_options={\"path\": \"s3://my-data-bucket/parquet/customers/\"},\n",
        "    format=\"parquet\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ **Output Location**\n",
        "\n",
        "```\n",
        "s3://my-data-bucket/parquet/customers/\n",
        "```\n",
        "\n",
        "## Q 12. Explain the difference between Kinesis Data Streams, Kinesis Firehose, and Kinesis Data Analytics. Provide a real-world example of how each would be used.\n",
        "Ans - ### ‚úÖ Kinesis Services ‚Äì Quick Comparison & Real-World Use Cases\n",
        "\n",
        "| Service                    | Purpose                                                               | Real-World Example                     |\n",
        "| -------------------------- | --------------------------------------------------------------------- | -------------------------------------- |\n",
        "| **Kinesis Data Streams**   | Real-time data ingestion (millisecond latency)                        | Ingest clickstream data from a website |\n",
        "| **Kinesis Firehose**       | Load streaming data to S3, Redshift, or Elasticsearch (fully managed) | Auto-deliver IoT data to S3 in Parquet |\n",
        "| **Kinesis Data Analytics** | Run SQL on streaming data (real-time processing)                      | Detect anomalies in stock prices live  |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary:\n",
        "\n",
        "* **Data Streams** = Fast, customizable **stream ingestion**\n",
        "* **Firehose** = Easy **stream-to-storage delivery**\n",
        "* **Data Analytics** = **SQL + stream processing** in real-time\n",
        "\n",
        "## Q 12. What is columnar storage and how does it benefit Redshift performance for analytics workloads.\n",
        "\n",
        "Ans - ### ‚úÖ What is Columnar Storage?\n",
        "\n",
        "**Columnar storage** stores data **by column** (not by row), so all values in a column are stored together.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö° Benefits for Redshift Analytics:\n",
        "\n",
        "| Benefit                | Why It Matters                             |\n",
        "| ---------------------- | ------------------------------------------ |\n",
        "| **Faster queries**     | Reads only needed columns, not entire rows |\n",
        "| **Better compression** | Similar column values compress more easily |\n",
        "| **Efficient scans**    | Ideal for large, read-heavy analytics      |\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Summary:\n",
        "\n",
        "Columnar storage in Redshift boosts performance by reducing I/O and improving compression‚Äîperfect for analytical workloads like BI dashboards or data warehousing.\n",
        "\n",
        "## Q 13. ### ‚úÖ Load CSV from S3 into Redshift (Short)\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Table Schema**\n",
        "\n",
        "```sql\n",
        "CREATE TABLE customers (\n",
        "  id INT,\n",
        "  name VARCHAR(50),\n",
        "  age INT\n",
        ");\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **COPY Command**\n",
        "\n",
        "```sql\n",
        "COPY customers\n",
        "FROM 's3://my-data-bucket/data/customers.csv'\n",
        "IAM_ROLE 'arn:aws:iam::123456789012:role/MyRedshiftRole'\n",
        "CSV\n",
        "IGNOREHEADER 1\n",
        "REGION 'us-east-1';\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Sample Query & Output**\n",
        "\n",
        "```sql\n",
        "SELECT * FROM customers LIMIT 3;\n",
        "```\n",
        "\n",
        "| id | name  | age |\n",
        "| -- | ----- | --- |\n",
        "| 1  | Alice | 30  |\n",
        "| 2  | Bob   | 25  |\n",
        "| 3  | Carol | 28  |\n",
        "\n",
        "\n",
        "## Q 14. What is the role of the AWS Glue Data Catalog in Athena? How does schema-on-read work\n",
        "\n",
        "Ans - ### ‚úÖ AWS Glue Data Catalog & Athena\n",
        "\n",
        "* **Glue Data Catalog**: Acts as a **central metadata repository** that stores table definitions, schemas, and locations of your data.\n",
        "* **Athena** uses this catalog to understand the structure of your data stored in S3 and run SQL queries without moving data.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö° Schema-on-Read\n",
        "\n",
        "* Data is **not transformed or structured upfront**.\n",
        "* Schema is applied **only when you run a query**.\n",
        "* Allows querying **raw data directly**, providing flexibility and faster ingestion.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Summary:\n",
        "\n",
        "Glue Data Catalog tells Athena **how to interpret the data**, while schema-on-read lets you query raw data dynamically without prior loading or transformation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R3bAS6ZG4k8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*    **Create an Athena table from S3 data using Glue Catalog. Run a query and share the SQL + result screenshot.**\n",
        "\n",
        "I can guide you through creating an Athena table using Glue Catalog and running a query, but I can‚Äôt generate screenshots directly. Here‚Äôs a quick walkthrough with the SQL you‚Äôd use:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Create Glue Crawler**\n",
        "\n",
        "* Point Glue Crawler to your S3 data location (e.g., `s3://my-data-bucket/data/`)\n",
        "* Run the crawler to create a table in Glue Data Catalog (e.g., database `mydb`, table `customers`)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Create Athena Table (If needed manually)**\n",
        "\n",
        "If you want to create manually (instead of crawler), here‚Äôs SQL to create an external table that uses Glue Catalog metadata:\n",
        "\n",
        "```sql\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS mydb.customers (\n",
        "  id int,\n",
        "  name string,\n",
        "  age int\n",
        ")\n",
        "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
        "WITH SERDEPROPERTIES (\n",
        "  \"separatorChar\" = \",\",\n",
        "  \"quoteChar\"     = \"\\\"\"\n",
        ")\n",
        "LOCATION 's3://my-data-bucket/data/'\n",
        "TBLPROPERTIES ('has_encrypted_data'='false');\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Run a Sample Query**\n",
        "\n",
        "```sql\n",
        "SELECT * FROM mydb.customers LIMIT 5;\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Sample Query Result**\n",
        "\n",
        "| id | name  | age |\n",
        "| -- | ----- | --- |\n",
        "| 1  | Alice | 30  |\n",
        "| 2  | Bob   | 25  |\n",
        "| 3  | Carol | 28  |\n",
        "| 4  | Dave  | 35  |\n",
        "| 5  | Eva   | 22  |\n",
        "\n",
        "\n",
        "**Describe how Amazon Quicksight supports business intelligence in a serverless data architecture. What are SPICE and embedded dashboards.**\n",
        "\n",
        "### Amazon QuickSight in Serverless BI (Short)\n",
        "\n",
        "* **QuickSight** is a fully **serverless BI service** that lets you create interactive dashboards and visualizations without managing infrastructure.\n",
        "* It integrates seamlessly with serverless data sources like **S3, Athena, and Redshift Spectrum**.\n",
        "* Automatically scales and charges per usage, making BI accessible and cost-efficient.\n",
        "\n",
        "---\n",
        "\n",
        "### What is SPICE?\n",
        "\n",
        "* **SPICE** is QuickSight‚Äôs **in-memory calculation engine** that speeds up data analysis by storing data in a distributed, fast cache.\n",
        "* Enables quick, scalable querying without hitting the original data source every time.\n",
        "\n",
        "---\n",
        "\n",
        "### What are Embedded Dashboards?\n",
        "\n",
        "* **Embedded dashboards** allow you to **integrate QuickSight visualizations inside your own apps or portals**.\n",
        "* Provide seamless, secure BI access to end-users without leaving your application.\n",
        "\n",
        "\n",
        " **Connect Quicksight to Athena or Redshift and build a dashboard with at least one calculated field and one filter. Share a screenshot of your final dashboard.**\n",
        "\n",
        " I can guide you quickly on how to do this, but I can‚Äôt generate or share actual screenshots. Here‚Äôs a short step-by-step:\n",
        "\n",
        "---\n",
        "\n",
        "### Quick Steps to Connect QuickSight ‚Üí Athena/Redshift & Build Dashboard\n",
        "\n",
        "1. **Connect Data Source**\n",
        "\n",
        "   * In QuickSight, go to **Manage data** ‚Üí **New data set**\n",
        "   * Choose **Athena** or **Redshift**\n",
        "   * Provide connection details and select your database/table\n",
        "\n",
        "2. **Prepare Data**\n",
        "\n",
        "   * Import or Direct Query your data\n",
        "   * Add a **calculated field**, e.g.\n",
        "     `Revenue_Per_Customer = total_revenue / customer_count`\n",
        "\n",
        "3. **Build Analysis**\n",
        "\n",
        "   * Create visuals (bar charts, tables, etc.)\n",
        "   * Add a **filter**, e.g., filter by `Region` or `Date`\n",
        "\n",
        "4. **Publish Dashboard**\n",
        "\n",
        "   * Save and publish your dashboard for sharing\n",
        "\n",
        "\n",
        "**Explain how AWS CloudWatch and CloudTrail differ. IN a data analytics pipeline, what role does each play in monitoring, auditing, and troubleshooting.**\n",
        "\n",
        "### AWS CloudWatch vs CloudTrail (Short)\n",
        "\n",
        "| Feature                       | CloudWatch                                      | CloudTrail                                                |\n",
        "| ----------------------------- | ----------------------------------------------- | --------------------------------------------------------- |\n",
        "| **Purpose**                   | Monitoring & operational metrics                | Auditing API calls & user activity                        |\n",
        "| **Data**                      | Logs, metrics, alarms                           | Records AWS API calls & events                            |\n",
        "| **Use case in data pipeline** | Track performance (latency, errors), set alerts | Audit who changed resources, track API usage for security |\n",
        "\n",
        "---\n",
        "\n",
        "### Roles in Data Analytics Pipeline\n",
        "\n",
        "* **CloudWatch**: Monitors pipeline health ‚Äî CPU usage, Lambda errors, processing delays; triggers alerts for failures.\n",
        "* **CloudTrail**: Audits user actions and API calls ‚Äî who started/stopped jobs, data access, configuration changes; helps troubleshoot security or compliance issues.\n",
        "\n",
        "\n",
        "**Describe a complete end-to-end data analytics pipeline using AWS services. Include services for data ingestion, storage, transformation, querying, and visualization. (Example: S3 ‚Üí Lambda ‚Üí Glue ‚Üí Quicksight)\n",
        " Explain why you would choose each service for the stage it‚Äôs used in.**\n",
        "\n",
        " Here‚Äôs a concise end-to-end AWS data analytics pipeline example:\n",
        "\n",
        "---\n",
        "\n",
        "### End-to-End Data Analytics Pipeline\n",
        "\n",
        "| Stage              | AWS Service           | Why Choose It?                                                                              |\n",
        "| ------------------ | --------------------- | ------------------------------------------------------------------------------------------- |\n",
        "| **Ingestion**      | **Amazon Kinesis**    | Real-time, scalable streaming ingestion for fast-moving data.                               |\n",
        "| **Storage**        | **Amazon S3**         | Durable, cost-effective object storage for raw and processed data.                          |\n",
        "| **Transformation** | **AWS Glue**          | Serverless ETL service to clean, enrich, and catalog data automatically.                    |\n",
        "| **Querying**       | **Amazon Athena**     | Serverless, pay-per-query SQL analytics directly on S3 data without ETL.                    |\n",
        "| **Visualization**  | **Amazon QuickSight** | Scalable, serverless BI tool that integrates easily with Athena for interactive dashboards. |\n",
        "\n",
        "---\n",
        "\n",
        "### Why this architecture?\n",
        "\n",
        "* **Kinesis** handles real-time ingestion with low latency.\n",
        "* **S3** stores data cheaply and reliably at scale.\n",
        "* **Glue** automates schema discovery & data transformation.\n",
        "* **Athena** enables flexible, on-demand querying without infrastructure.\n",
        "* **QuickSight** empowers users with fast, shareable visual insights without setup overhead.\n"
      ],
      "metadata": {
        "id": "FlcyMhPg8Ka5"
      }
    }
  ]
}